# LLM Evaluation Platform

Welcome to the LLM Evaluation Platform! This full-stack web application allows you to input prompts and view responses from multiple Large Language Models (LLMs) side-by-side. The platform integrates metrics such as accuracy, relevancy, response time, latency, and coherence for each LLM, providing a comprehensive evaluation of their performance.

## Features

- **Prompt Input**: Easily input prompts and get responses from various LLMs.
- **Response Comparison**: View and compare responses from models like Gemini, GPT-4, Mixtral, and Llama.
- **Performance Metrics**: Analyze metrics including accuracy, relevancy, response time, latency, and coherence.
- **Analytics Dashboard**: Visualize performance metrics for different prompts and LLMs.
- **Database Integration**: Store user prompts and experiment results using Supabase.

## Technologies Used

- **Frontend**: React, TypeScript, Tailwind CSS, Recharts
- **Backend**: Supabase, Google Generative AI, OpenAI, Groq SDK
- **Build Tools**: Vite, ESLint, PostCSS

## Getting Started

### Prerequisites

- Node.js (v14 or higher)
- npm or yarn

### Installation

1. Clone the repository:
   ```sh
   git clone https://github.com/yourusername/llm-evaluation.git
   cd llm-evaluation
